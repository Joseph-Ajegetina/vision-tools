{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CINIC-10 MLP Training Notebook\n",
    "\n",
    "This notebook demonstrates training a Multi-Layer Perceptron (MLP) on the CINIC-10 dataset.\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "The MLP processes flattened images through fully connected layers:\n",
    "- **Input**: 32×32×3 = 3072 features (flattened RGB image)\n",
    "- **Hidden layers**: Apply linear transformation followed by ReLU activation\n",
    "- **Forward pass**: `y = f(Wx + b)` where `f` is ReLU\n",
    "- **Loss**: Cross-entropy loss for multi-class classification\n",
    "- **Optimization**: Adam optimizer with backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Import project modules\n",
    "from src.models.mlp import MLP\n",
    "from src.data.dataset import CINIC10DataModule\n",
    "from src.training.trainer import ModelTrainer\n",
    "from src.training.evaluator import ModelEvaluator\n",
    "from src.utils.export import ModelExporter\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "with open('../configs/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(config['seed'])\n",
    "np.random.seed(config['seed'])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(config['seed'])\n",
    "\n",
    "print(\"Configuration loaded and random seeds set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Setup\n",
    "\n",
    "Load and prepare the CINIC-10 dataset with appropriate preprocessing for MLP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data module\n",
    "data_module = CINIC10DataModule(\n",
    "    data_dir=config['dataset']['data_dir'],\n",
    "    batch_size=config['data_loader']['batch_size'],\n",
    "    num_workers=config['data_loader']['num_workers'],\n",
    "    pin_memory=config['data_loader']['pin_memory'],\n",
    "    validation_split=config['data_loader']['validation_split'],\n",
    "    seed=config['seed']\n",
    ")\n",
    "\n",
    "# Setup data loaders\n",
    "print(\"Setting up data loaders...\")\n",
    "data_loaders = data_module.setup_data_loaders(use_augmentation=True)\n",
    "\n",
    "# Display dataset information\n",
    "dataset_info = data_module.get_dataset_info()\n",
    "print(\"\\nDataset Information:\")\n",
    "for key, value in dataset_info.items():\n",
    "    if key != 'class_names':\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nClass names: {dataset_info['class_names']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample data\n",
    "print(\"Visualizing sample data...\")\n",
    "data_module.visualize_samples(num_samples=8, split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MLP Model Architecture\n",
    "\n",
    "Create and examine the MLP model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MLP model\n",
    "mlp_model = MLP(\n",
    "    input_size=3072,  # 32 * 32 * 3 (flattened CINIC-10 image)\n",
    "    hidden_layers=config['models']['mlp']['hidden_layers'],\n",
    "    num_classes=config['dataset']['num_classes'],\n",
    "    dropout=config['models']['mlp']['dropout'],\n",
    "    activation=config['models']['mlp']['activation']\n",
    ")\n",
    "\n",
    "# Display model information\n",
    "print(\"MLP Model Architecture:\")\n",
    "print(mlp_model.summary())\n",
    "\n",
    "print(\"\\nDetailed Model Information:\")\n",
    "model_info = mlp_model.get_model_info()\n",
    "for key, value in model_info.items():\n",
    "    if key != 'mathematical_foundation':\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nMathematical Foundation:\")\n",
    "for key, value in model_info['mathematical_foundation'].items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Process\n",
    "\n",
    "Train the MLP model with comprehensive monitoring and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "mlp_trainer = ModelTrainer(\n",
    "    model=mlp_model,\n",
    "    device=device,\n",
    "    config=config,\n",
    "    experiment_name=\"MLP_CINIC10\"\n",
    ")\n",
    "\n",
    "print(\"Starting MLP training...\")\n",
    "print(f\"Training for {config['training']['epochs']} epochs\")\n",
    "print(f\"Learning rate: {config['training']['learning_rate']}\")\n",
    "print(f\"Optimizer: {config['training']['optimizer']}\")\n",
    "print(f\"Scheduler: {config['training']['scheduler']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "training_history = mlp_trainer.train(\n",
    "    train_loader=data_loaders['train'],\n",
    "    val_loader=data_loaders['val'],\n",
    "    save_checkpoints=True\n",
    ")\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(f\"Best validation accuracy: {mlp_trainer.best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "mlp_trainer.plot_training_history(save_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation\n",
    "\n",
    "Comprehensive evaluation of the trained MLP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator\n",
    "evaluator = ModelEvaluator(\n",
    "    class_names=data_module.class_names,\n",
    "    device=device,\n",
    "    save_dir=\"./mlp_evaluation_results\"\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluating MLP model...\")\n",
    "mlp_results = evaluator.evaluate_model(\n",
    "    model=mlp_model,\n",
    "    test_loader=data_loaders['test'],\n",
    "    model_name=\"MLP\"\n",
    ")\n",
    "\n",
    "# Display overall results\n",
    "print(\"\\nMLP Evaluation Results:\")\n",
    "print(f\"Overall Accuracy: {mlp_results['overall_metrics']['accuracy']:.2f}%\")\n",
    "print(f\"Top-2 Accuracy: {mlp_results['overall_metrics']['top2_accuracy']:.2f}%\")\n",
    "print(f\"Top-3 Accuracy: {mlp_results['overall_metrics']['top3_accuracy']:.2f}%\")\n",
    "print(f\"Macro F1-Score: {mlp_results['overall_metrics']['macro_f1']:.2f}%\")\n",
    "print(f\"Weighted F1-Score: {mlp_results['overall_metrics']['weighted_f1']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "evaluator.plot_confusion_matrix(mlp_results, normalize=True, save_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class performance analysis\n",
    "print(\"Per-Class Performance:\")\n",
    "per_class = mlp_results['per_class_metrics']\n",
    "\n",
    "for i, class_name in enumerate(per_class['class_names']):\n",
    "    print(f\"{class_name:12s}: Acc={per_class['accuracy'][i]:5.1f}% | \"\n",
    "          f\"Prec={per_class['precision'][i]:5.1f}% | \"\n",
    "          f\"Rec={per_class['recall'][i]:5.1f}% | \"\n",
    "          f\"F1={per_class['f1_score'][i]:5.1f}% | \"\n",
    "          f\"Support={per_class['support'][i]:4d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Export for Deployment\n",
    "\n",
    "Export the trained model in multiple formats for production deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model exporter\n",
    "exporter = ModelExporter(export_dir=\"../exported_models\")\n",
    "\n",
    "# Export model in all formats\n",
    "print(\"Exporting MLP model...\")\n",
    "export_results = exporter.export_all_formats(\n",
    "    model=mlp_model,\n",
    "    model_name=\"MLP\",\n",
    "    input_shape=(1, 3, 32, 32),\n",
    "    config={\n",
    "        'onnx': {'opset_version': 11, 'verify': True},\n",
    "        'torchscript': {'method': 'trace', 'verify': True},\n",
    "        'state_dict': {'include_metadata': True}\n",
    "    }\n",
    ")\n",
    "\n",
    "# Display export results\n",
    "print(\"\\nExport Results:\")\n",
    "for format_name, result in export_results['exports'].items():\n",
    "    if 'error' not in result:\n",
    "        print(f\"{format_name.upper():15s}: ✓ Success - {result['file_size_mb']:.2f} MB\")\n",
    "        if 'verification' in result and result['verification']['verified']:\n",
    "            print(f\"{'':15s}  Verification: ✓ Outputs match (max diff: {result['verification'].get('max_difference', 'N/A')})\")\n",
    "    else:\n",
    "        print(f\"{format_name.upper():15s}: ✗ Failed - {result['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analysis and Insights\n",
    "\n",
    "Analyze the MLP model's performance and characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model analysis\n",
    "print(\"MLP Model Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n1. Architecture Characteristics:\")\n",
    "print(f\"   - Input features: 3,072 (32×32×3 flattened)\")\n",
    "print(f\"   - Hidden layers: {config['models']['mlp']['hidden_layers']}\")\n",
    "print(f\"   - Total parameters: {mlp_model.count_parameters():,}\")\n",
    "print(f\"   - Model size: {mlp_model.get_parameter_size_mb():.2f} MB\")\n",
    "\n",
    "print(\"\\n2. Performance Summary:\")\n",
    "print(f\"   - Test accuracy: {mlp_results['overall_metrics']['accuracy']:.2f}%\")\n",
    "print(f\"   - Best validation accuracy: {mlp_trainer.best_val_acc:.2f}%\")\n",
    "print(f\"   - Training epochs: {len(training_history['train_loss'])}\")\n",
    "\n",
    "if 'inference_time' in mlp_results:\n",
    "    print(f\"   - Average inference time: {mlp_results['inference_time']['mean_ms']:.2f} ms\")\n",
    "\n",
    "print(\"\\n3. Mathematical Insights:\")\n",
    "print(\"   - Flattens spatial information into 1D vector\")\n",
    "print(\"   - Relies on global patterns rather than local features\")\n",
    "print(\"   - Uses ReLU activation for non-linearity\")\n",
    "print(\"   - Dropout regularization prevents overfitting\")\n",
    "\n",
    "# Find best and worst performing classes\n",
    "accuracies = mlp_results['per_class_metrics']['accuracy']\n",
    "best_class_idx = np.argmax(accuracies)\n",
    "worst_class_idx = np.argmin(accuracies)\n",
    "\n",
    "print(\"\\n4. Class Performance:\")\n",
    "print(f\"   - Best class: {data_module.class_names[best_class_idx]} ({accuracies[best_class_idx]:.1f}%)\")\n",
    "print(f\"   - Worst class: {data_module.class_names[worst_class_idx]} ({accuracies[worst_class_idx]:.1f}%)\")\n",
    "print(f\"   - Performance range: {max(accuracies) - min(accuracies):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Results and Summary\n",
    "\n",
    "Save all results for comparison with CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive report\n",
    "report = evaluator.generate_report([mlp_results], save_report=True)\n",
    "print(\"Comprehensive evaluation report:\")\n",
    "print(report)\n",
    "\n",
    "# Save training summary\n",
    "training_summary = mlp_trainer.get_summary()\n",
    "print(\"\\nTraining Summary:\")\n",
    "for key, value in training_summary.items():\n",
    "    if key != 'config':\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MLP training and evaluation completed successfully!\")\n",
    "print(\"Next: Run CNN training notebook for comparison\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
